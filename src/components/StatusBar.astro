---
/**
 * StatusBar Component
 * Bottom navigation bar with vim-style status indicators
 */
export interface StatusBarProps {
  className?: string;
}

const { className = '' } = Astro.props as StatusBarProps;
---

<nav class={`status-bar ${className}`} role="navigation" aria-label="Section Navigation">
  <div class="status-left">
    <div class="status-mode" id="statusMode">NORMAL</div>
    <div class="status-scheme" id="statusScheme">Matrix Rain</div>
    <div class="status-file">portfolio.md</div>
  </div>
  <div class="status-center">
    <a href="#contact-info" class="status-item" aria-label="Navigate to Contact section" tabindex="0">
      <span class="status-text">Contact</span>
    </a>
    <div class="status-separator">|</div>
    <a href="#project-manifest" class="status-item" aria-label="Navigate to Projects section" tabindex="0">
      <span class="status-text">Projects</span>
    </a>
    <div class="status-separator">|</div>
    <a href="#skill-matrix" class="status-item" aria-label="Navigate to Skills section" tabindex="0">
      <span class="status-text">Skills</span>
    </a>
    <div class="status-separator">|</div>
    <a href="#education-log" class="status-item" aria-label="Navigate to Education section" tabindex="0">
      <span class="status-text">Education</span>
    </a>
  </div>
  <div class="status-right">
    <div class="status-position">1:1</div>
    <div class="status-progress-container" role="progressbar" aria-label="Page scroll progress" aria-valuemin="0" aria-valuemax="100" aria-valuenow="0">
      <svg width="20" height="20" viewBox="0 0 60 60" aria-hidden="true">
        <circle cx="30" cy="30" r="25" fill="none" stroke="var(--muted-digital)" stroke-width="2"></circle>
        <circle class="progress-ring" cx="30" cy="30" r="25" fill="none" stroke="var(--foreground-digital)" stroke-width="2" stroke-linecap="round" transform="rotate(-90 30 30)"></circle>
      </svg>
      <span class="progress-percentage" aria-live="polite">100%</span>
    </div>
    <button class="scroll-to-top-btn status-button" id="scrollToTopBtn" aria-label="Scroll to top" data-show-after="300">
      <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5">
        <path d="m18 15-6-6-6 6"/>
      </svg>
    </button>
    <button class="voice-call-btn status-button" id="voiceCallBtn" aria-label="Start voice conversation with Ali">
      <svg id="voiceCallIcon" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5">
        <path d="M12 14C13.66 14 15 12.66 15 11V5C15 3.34 13.66 2 12 2C10.34 2 9 3.34 9 5V11C9 12.66 10.34 14 12 14Z"/>
        <path d="M17 11C17 14.53 14.39 17.44 11 17.93V21H13C13.55 21 14 21.45 14 22C14 22.55 13.55 23 13 23H11C10.45 23 10 22.55 10 22C10 21.45 10.45 21 11 21H11V17.93C7.61 17.44 5 14.53 5 11H7C7 13.76 9.24 16 12 16C14.76 16 17 13.76 17 11H17Z"/>
      </svg>
    </button>
    <button class="view-switcher status-button" aria-label="Toggle between digital and print view">
      <svg class="icon-digital" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5">
        <rect x="2" y="3" width="20" height="14" rx="2" ry="2"></rect>
        <line x1="8" y1="21" x2="16" y2="21"></line>
        <line x1="12" y1="17" x2="12" y2="21"></line>
      </svg>
      <svg class="icon-print" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5">
        <polyline points="6,9 6,2 18,2 18,9"></polyline>
        <path d="M6,18H4a2,2,0,0,1-2-2V11a2,2,0,0,1,2,2H20a2,2,0,0,1,2-2v5a2,2,0,0,1-2,2H18"></path>
        <rect x="6" y="14" width="12" height="8"></rect>
      </svg>
    </button>
  </div>
</nav>

<style>

/* Status Bar Styles */
.status-bar { 
  position: fixed; 
  bottom: 0;
  left: 0;
  right: 0;
  height: 32px;
  display: flex; 
  align-items: center;
  justify-content: space-between;
  z-index: var(--z-sticky); 
  backdrop-filter: blur(8px); 
  background: var(--background-digital); 
  border-top: 1px solid var(--border-digital); 
  box-shadow: var(--shadow-lg);
  font-family: var(--font-body);
  font-size: var(--font-size-sm);
  padding: 0 var(--space-md);
}

/* Print Mode Status Bar */
body.print-view .status-bar {
  background: var(--background-print);
  border-top: 1px solid var(--border-print);
  color: var(--foreground-print);
  box-shadow: var(--shadow-sm);
}

/* Hide digital-only elements in print mode */
body.print-view .status-mode,
body.print-view .status-scheme,
body.print-view .status-position,
body.print-view .status-progress-container {
  display: none;
}

/* Show only navigation and mode switcher in print mode */
body.print-view .status-left {
  display: none;
}

body.print-view .status-center {
  flex: 1;
  justify-content: center;
}

body.print-view .status-right {
  display: flex;
}

/* Print mode navigation styling */
body.print-view .status-item {
  color: var(--muted-print);
}

body.print-view .status-item:hover {
  color: var(--foreground-print);
  background: var(--accent-print);
}

body.print-view .status-item.active {
  color: var(--background-print);
  background: var(--foreground-print);
  box-shadow: var(--shadow-sm);
}

body.print-view .status-separator {
  color: var(--muted-print);
  opacity: 0.8;
}

.status-left {
  display: flex;
  align-items: center;
  gap: var(--space-sm);
  color: var(--foreground-digital);
}

.status-mode {
  background: var(--foreground-digital);
  color: var(--background-digital);
  padding: var(--space-2xs) var(--space-sm);
  font-weight: var(--font-weight-bold);
  font-size: var(--font-size-sm);
  text-transform: uppercase;
  letter-spacing: var(--letter-spacing-wide);
  transition: all var(--transition-base) var(--ease-in-out);
}

.status-mode.insert {
  background: var(--color-success);
  color: var(--background-digital);
}

.status-mode.visual {
  background: var(--color-info);
  color: var(--background-digital);
}

.status-mode.command {
  background: var(--color-error);
  color: var(--background-print);
}

.status-file {
  color: var(--muted-digital);
  font-weight: var(--font-weight-medium);
}

.status-scheme {
  color: var(--foreground-digital);
  font-weight: var(--font-weight-normal);
  font-size: var(--font-size-xs);
  opacity: 0.8;
  padding: 1px var(--space-xs);
  border: 1px solid var(--border-digital);
  border-radius: var(--radius-sm);
}

.status-center {
  display: flex;
  align-items: center;
  gap: var(--space-sm);
  flex: 1;
  justify-content: center;
}

.status-item { 
  color: var(--muted-digital); 
  text-decoration: none; 
  font-size: var(--font-size-sm); 
  padding: var(--space-xs) var(--space-sm); 
  border-radius: var(--radius-base); 
  transition: all var(--transition-fast) var(--ease-in-out);
  display: flex;
  align-items: center;
  gap: var(--space-xs);
  font-weight: var(--font-weight-medium);
}

.status-text {
  font-weight: var(--font-weight-medium);
  text-transform: uppercase;
  letter-spacing: var(--letter-spacing-wide);
}

.status-separator {
  color: var(--muted-digital);
  font-weight: var(--font-weight-normal);
  opacity: 0.6;
}

.status-item:hover { 
  color: var(--foreground-digital); 
  background: var(--accent-digital);
}

.status-item.active { 
  color: var(--background-digital); 
  background: var(--foreground-digital); 
  box-shadow: 0 2px 8px var(--color-focus-ring);
}

.status-right {
  display: flex;
  align-items: center;
  gap: var(--space-sm);
  color: var(--muted-digital);
  font-size: var(--font-size-sm);
}

.status-position {
  color: var(--foreground-digital);
  font-weight: var(--font-weight-semibold);
}

.status-progress-container {
  display: flex;
  align-items: center;
  gap: var(--space-xs);
  color: var(--muted-digital);
  font-weight: var(--font-weight-medium);
}

.status-progress-container svg {
  width: 16px;
  height: 16px;
}

.status-progress-container .progress-percentage {
  font-size: var(--font-size-sm);
  color: var(--muted-digital);
}

/* Progress Ring Animation */
.progress-ring {
  stroke-dasharray: 157; /* 2 * Ï€ * 25 */
  stroke-dashoffset: 157;
  transition: stroke-dashoffset var(--transition-base) var(--ease-out);
}

/* Status Button (View Switcher) */
.status-button { 
  background: transparent; 
  border: none; 
  color: var(--muted-digital); 
  height: 24px; 
  width: 24px; 
  border-radius: var(--radius-sm); 
  cursor: pointer; 
  display: inline-flex; 
  align-items: center; 
  justify-content: center; 
  transition: all var(--transition-fast) var(--ease-in-out); 
  font-size: var(--font-size-sm);
  font-weight: var(--font-weight-medium);
  padding: 0;
}

.status-button svg { 
  width: 16px; 
  height: 16px; 
  transition: all var(--transition-fast) var(--ease-in-out); 
}

body.digital-view .icon-print { display: none; }
body.print-view .icon-digital { display: none; }

body.digital-view .icon-digital { 
  stroke: var(--muted-digital); 
  stroke-width: 1.5; 
  fill: none; 
}

body.digital-view .status-button:hover { 
  color: var(--foreground-digital); 
  background: var(--accent-digital); 
}

body.digital-view .status-button:hover .icon-digital { 
  stroke: var(--foreground-digital); 
  stroke-width: 2; 
}

body.digital-view .status-button:active { 
  color: var(--background-digital); 
  background: var(--foreground-digital); 
  box-shadow: 0 2px 8px var(--color-focus-ring);
}

body.print-view .status-button { 
  color: var(--muted-print); 
}

body.print-view .icon-print { 
  stroke: var(--muted-print); 
  stroke-width: 1.5; 
  fill: none; 
}

body.print-view .status-button:hover { 
  color: var(--foreground-print); 
  background: var(--accent-print); 
}

body.print-view .status-button:hover .icon-print { 
  stroke: var(--foreground-print); 
  stroke-width: 2; 
}

/* Voice Call Button */
.voice-call-btn.active {
  color: var(--color-success);
  animation: pulse 2s infinite;
}

.voice-call-btn.processing {
  color: var(--color-warning);
  animation: pulse 1s infinite;
}

.voice-call-btn.error {
  color: var(--color-error);
}

@keyframes pulse {
  0%, 100% { opacity: 1; }
  50% { opacity: 0.7; }
}

/* Scroll to Top Button */
.scroll-to-top-btn {
  opacity: 0;
  visibility: hidden;
  transform: scale(0.8);
  transition: all var(--transition-base) var(--ease-in-out);
}

.scroll-to-top-btn.visible {
  opacity: 1;
  visibility: visible;
  transform: scale(1);
}

.scroll-to-top-btn:hover {
  transform: scale(1.1);
}

/* Enhanced mobile-first responsive adjustments with fluid sizing */
@media (max-width: 768px) {
  .status-bar { 
    height: clamp(32px, 6vw, 40px);
    padding: 0 var(--space-sm);
    font-size: var(--font-size-sm);
    /* Enhanced touch targets */
    min-height: var(--touch-target-min);
  }
  
  .status-left { gap: var(--space-sm); }
  .status-mode { 
    padding: var(--space-2xs) var(--space-xs); 
    font-size: var(--font-size-xs); 
    min-height: var(--touch-target-min);
  }
  .status-center { gap: var(--space-xs); }
  .status-item { 
    font-size: var(--font-size-xs); 
    padding: var(--space-2xs) var(--space-xs);
    min-height: var(--touch-target-min);
    min-width: var(--touch-target-min);
  }
  .status-right { gap: var(--space-sm); font-size: var(--font-size-xs); }
  
  .status-button { 
    height: clamp(20px, 4vw, 24px); 
    width: clamp(20px, 4vw, 24px); 
    min-height: var(--touch-target-min);
    min-width: var(--touch-target-min);
  }
  
  .status-button svg {
    width: clamp(12px, 3vw, 16px);
    height: clamp(12px, 3vw, 16px);
  }
}

@media (max-width: 414px) {
  .status-bar {
    height: clamp(28px, 5vw, 36px);
    padding: 0 var(--space-xs);
    font-size: var(--font-size-xs);
  }
  
  .status-left { gap: var(--space-xs); }
  .status-mode { 
    padding: var(--space-2xs) var(--space-2xs); 
    font-size: var(--font-size-2xs); 
  }
  .status-center { gap: var(--space-2xs); }
  .status-item { 
    font-size: var(--font-size-2xs); 
    padding: var(--space-2xs) var(--space-xs);
  }
  .status-right { gap: var(--space-xs); font-size: var(--font-size-2xs); }
}

@media (max-width: 375px) {
  .status-file { display: none; }
  .status-text { display: none; }
  .status-scheme { display: none; } /* Hide scheme on very small screens */
  
  /* Print mode mobile adjustments */
  body.print-view .status-text {
    display: inline;
  }
  
  .status-button {
    height: clamp(18px, 4vw, 22px);
    width: clamp(18px, 4vw, 22px);
  }
  
  .status-button svg {
    width: clamp(10px, 2.5vw, 14px);
    height: clamp(10px, 2.5vw, 14px);
  }
}

@media (max-width: 320px) {
  .status-bar {
    height: clamp(24px, 4vw, 32px);
    padding: 0 var(--space-2xs);
  }
  
  .status-left { 
    gap: var(--space-2xs); 
    flex: 0 1 auto;
    overflow: hidden;
  }
  .status-center { 
    gap: var(--space-2xs);
    flex: 1 1 0;
    justify-content: center;
  }
  .status-right { 
    gap: var(--space-2xs);
    flex: 0 1 auto;
  }
  
  .status-mode {
    font-size: var(--font-size-2xs);
    padding: 1px var(--space-2xs);
    white-space: nowrap;
    overflow: hidden;
    text-overflow: ellipsis;
    max-width: 60px;
  }
}
</style>

<script>
// Status Bar Navigation Handler
const initStatusBarNavigation = () => {
  const statusBar = document.querySelector('.status-bar');
  if (!statusBar) return;
  
  // Use event delegation for better performance
  statusBar.addEventListener('click', (e) => {
    const target = e.target as HTMLElement;
    const statusItem = target.closest('.status-item') as HTMLAnchorElement;
    
    if (!statusItem) return;
    
    e.preventDefault();
    
    const targetId = statusItem.getAttribute('href')?.substring(1);
    const targetElement = targetId ? document.getElementById(targetId) : null;
    
    if (targetElement) {
      // Use requestAnimationFrame for smoother scrolling
      requestAnimationFrame(() => {
        targetElement.scrollIntoView({ behavior: 'smooth' });
      });
    }
  });
};

// View Switcher Handler
const initViewSwitcher = () => {
  const switcher = document.querySelector('.view-switcher') as HTMLElement | null;
  const body = document.body;
  
  if (switcher) {
    switcher.addEventListener('click', () => {
      if (body.classList.contains('digital-view')) {
        body.classList.remove('digital-view');
        body.classList.add('print-view');
      } else {
        body.classList.remove('print-view');
        body.classList.add('digital-view');
      }
    });
  }
};

// Scroll to Top Handler
const initScrollToTop = () => {
  const scrollToTopBtn = document.querySelector('#scrollToTopBtn') as HTMLElement | null;
  
  if (!scrollToTopBtn) return;
  
  const showAfter = parseInt(scrollToTopBtn.dataset['showAfter'] || '300', 10);
  
  // Throttle function for performance
  function throttle(func: Function, limit: number) {
    let inThrottle: boolean;
    return function(this: any, ...args: any[]) {
      if (!inThrottle) {
        func.apply(this, args);
        inThrottle = true;
        setTimeout(() => inThrottle = false, limit);
      }
    };
  }
  
  function toggleScrollToTop() {
    try {
      const scrollTop = window.pageYOffset || document.documentElement.scrollTop;
      if (scrollToTopBtn) {
        if (scrollTop > showAfter) {
          scrollToTopBtn.classList.add('visible');
        } else {
          scrollToTopBtn.classList.remove('visible');
        }
      }
    } catch (error) {
      console.error('Failed to toggle scroll to top button:', error);
    }
  }
  
  const scrollToTop = () => {
    try {
      window.scrollTo({ top: 0, behavior: 'smooth' });
    } catch (error) {
      // Fallback for browsers that don't support smooth scrolling
      console.warn('Smooth scrolling not supported, using fallback:', error);
      window.scrollTo(0, 0);
    }
  };
  
  // Event listeners
  scrollToTopBtn.addEventListener('click', scrollToTop);
  
  // Add keyboard support
  scrollToTopBtn.addEventListener('keydown', (e) => {
    const keyEvent = e as KeyboardEvent;
    if (keyEvent.key === 'Enter' || keyEvent.key === ' ') {
      keyEvent.preventDefault();
      scrollToTop();
    }
  });
  
  // Use throttled scroll listener for better performance
  const throttledToggleScrollToTop = throttle(toggleScrollToTop, 100);
  
  window.addEventListener('scroll', throttledToggleScrollToTop, { passive: true });
  
  // Initial check
  toggleScrollToTop();
};

// Production-ready Full-Duplex Voice Call Handler with Interruption Support
async function initVoiceCall() {
  const voiceButton = document.getElementById('voiceCallBtn') as HTMLButtonElement | null;
  const voiceIcon = document.getElementById('voiceCallIcon') as SVGElement | null;
  
  if (!voiceButton || !voiceIcon) return;
  
  // Check initial voice backend status
  try {
    const configResponse = await fetch('/api/voice/config');
    const voiceConfig = await configResponse.json();
    
    if (!voiceConfig.voiceEnabled || !voiceConfig.voiceBackendRunning) {
      voiceButton.style.opacity = '0.5';
      voiceButton.title = voiceConfig.instructions || 'Voice backend not available';
    }
  } catch (error) {
    console.warn('Could not check initial voice status:', error);
    voiceButton.style.opacity = '0.5';
    voiceButton.title = 'Voice status unknown - click to try';
  }
  
  let ws: WebSocket | null = null;
  let audioStream: MediaStream | null = null;
  let inputAudioContext: AudioContext | null = null;
  let outputAudioContext: AudioContext | null = null;
  let voiceProcessor: AudioWorkletNode | null = null;
  let isCallActive = false;
  let isConnecting = false;
  let currentState: 'idle' | 'listening' | 'user_speaking' | 'ai_speaking' | 'interrupting' = 'idle';
  let isUserSpeaking = false;
  let isAISpeaking = false;
  
  function updateButtonState(state: 'idle' | 'active' | 'processing' | 'error') {
    voiceButton.className = `voice-call-btn status-button ${state}`;
  }
  
  function showStatusMessage(message: string) {
    const statusMode = document.getElementById('statusMode');
    if (statusMode) {
      statusMode.textContent = message;
    }
  }
  
  async function startRealtimeCall() {
    if (isConnecting || isCallActive) return;
    
    try {
      isConnecting = true;
      updateButtonState('processing');
      showStatusMessage('CONNECTING');
      
      // Check voice backend status via API
      let voiceConfig;
      try {
        const configResponse = await fetch('/api/voice/config');
        voiceConfig = await configResponse.json();
        
        if (!voiceConfig.voiceEnabled) {
          showStatusMessage('VOICE DISABLED');
          updateButtonState('error');
          isConnecting = false;
          return;
        }
        
        if (!voiceConfig.voiceBackendRunning) {
          showStatusMessage('BACKEND OFFLINE');
          updateButtonState('error');
          isConnecting = false;
          
          // Reset to normal after showing error
          setTimeout(() => {
            showStatusMessage('NORMAL');
            updateButtonState('idle');
          }, 3000);
          return;
        }
        
        // Reset button appearance if backend is available
        voiceButton.style.opacity = '1';
        voiceButton.title = 'Start voice conversation with Ali';
      } catch (error) {
        console.error('Failed to check voice config:', error);
        showStatusMessage('CONFIG ERROR');
        updateButtonState('error');
        isConnecting = false;
        
        // Reset to normal after showing error
        setTimeout(() => {
          showStatusMessage('NORMAL');
          updateButtonState('idle');
        }, 3000);
        return;
      }
      
      // Get microphone access
      audioStream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: 24000
        }
      });
      
      // Setup separate audio contexts for input and output
      inputAudioContext = new AudioContext({ sampleRate: 24000 });
      outputAudioContext = new AudioContext({ sampleRate: 24000 });
      
      // Load and setup AudioWorklet for low-latency processing
      await inputAudioContext.audioWorklet.addModule('/audio-worklet.js');
      
      const source = inputAudioContext.createMediaStreamSource(audioStream);
      voiceProcessor = new AudioWorkletNode(inputAudioContext, 'voice-processor');
      
      // Connect to WebSocket server using configured URL
      const wsUrl = voiceConfig.voiceWsUrl || 'ws://localhost:3001';
      
      ws = new WebSocket(wsUrl);
      
      ws.onopen = () => {
        isConnecting = false;
        isCallActive = true;
        currentState = 'listening';
        updateButtonState('active');
        showStatusMessage('CONNECTED');
      };
      
      ws.onmessage = (event) => {
        try {
          const message = JSON.parse(event.data);
          handleOpenAIMessage(message);
        } catch (error) {
          console.error('Error parsing WebSocket message:', error);
        }
      };
      
      ws.onerror = (error) => {
        console.error('WebSocket error:', error);
        showStatusMessage('CONNECTION FAILED');
        updateButtonState('error');
        endCall();
        
        // Show user-friendly error message after a delay
        setTimeout(() => {
          showStatusMessage('VOICE UNAVAILABLE');
        }, 2000);
      };
      
      ws.onclose = () => {
        if (isCallActive) {
          showStatusMessage('DISCONNECTED');
          endCall();
        }
      };
      
      // Setup AudioWorklet message handling for speech detection and audio streaming
      voiceProcessor.port.onmessage = (event) => {
        const { type, data, isSpeechDetected, timestamp } = event.data;
        
        switch (type) {
          case 'speech-start':
            handleUserSpeechStart();
            break;
            
          case 'speech-end':
            handleUserSpeechEnd();
            break;
            
          case 'audio-data':
            if (ws && ws.readyState === WebSocket.OPEN && isCallActive && !isAISpeaking) {
              ws.send(JSON.stringify({
                type: 'input_audio_buffer.append',
                audio: arrayBufferToBase64(data.buffer)
              }));
            }
            break;
        }
      };
      
      source.connect(voiceProcessor);
      
    } catch (error) {
      console.error('Error starting realtime call:', error);
      
      // Provide specific error messages based on error type
      if (error.name === 'NotAllowedError') {
        showStatusMessage('MIC DENIED');
      } else if (error.name === 'NotFoundError') {
        showStatusMessage('NO MICROPHONE');
      } else if (error.name === 'AbortError') {
        showStatusMessage('CONNECTION TIMEOUT');
      } else {
        showStatusMessage('VOICE ERROR');
      }
      
      updateButtonState('error');
      isConnecting = false;
      endCall();
      
      // Reset to normal after showing error
      setTimeout(() => {
        showStatusMessage('NORMAL');
        updateButtonState('idle');
      }, 3000);
    }
  }
  
  function handleOpenAIMessage(message: any) {
    switch (message.type) {
      case 'session.created':
        currentState = 'listening';
        showStatusMessage('READY');
        break;
        
      case 'input_audio_buffer.speech_started':
        if (!isUserSpeaking) {
          currentState = 'user_speaking';
          showStatusMessage('LISTENING');
        }
        break;
        
      case 'input_audio_buffer.speech_stopped':
        if (currentState === 'user_speaking') {
          currentState = 'listening';
          showStatusMessage('PROCESSING');
        }
        break;
        
      case 'conversation.item.input_audio_transcription.completed':
        const transcript = message.transcript;
        if (transcript?.length > 3) {
          showStatusMessage(`"${transcript.substring(0, 20)}..."`);
        }
        break;
        
      case 'response.audio_transcript.delta':
        // Show AI response text as it's being generated
        break;
        
      case 'response.audio.delta':
        // Only play if user is not speaking (prevents interruption issues)
        if (message.delta && !isUserSpeaking) {
          if (!isAISpeaking) {
            isAISpeaking = true;
            currentState = 'ai_speaking';
          }
          playAudioChunk(message.delta);
        }
        break;
        
      case 'response.done':
        isAISpeaking = false;
        currentState = 'listening';
        showStatusMessage('LISTENING');
        break;
        
      case 'error':
        console.error('OpenAI Realtime API error:', message);
        showStatusMessage('ERROR');
        break;
        
      default:
    }
  }
  
  let audioQueue: Float32Array[] = [];
  let isPlayingAudio = false;
  let audioStartTime = 0;
  let currentAudioSource: AudioBufferSourceNode | null = null;
  
  function playAudioChunk(audioBase64: string) {
    try {
      // Don't queue new audio if user is speaking (interruption handling)
      if (isUserSpeaking) {
        return;
      }
      
      const audioData = base64ToArrayBuffer(audioBase64);
      const int16Array = new Int16Array(audioData);
      const float32Array = new Float32Array(int16Array.length);
      
      // Convert to float32
      for (let i = 0; i < int16Array.length; i++) {
        float32Array[i] = int16Array[i] / 32768;
      }
      
      audioQueue.push(float32Array);
      
      if (!isPlayingAudio && !isUserSpeaking) {
        playQueuedAudio();
      }
    } catch (error) {
      console.error('Error processing audio chunk:', error);
    }
  }
  
  function playQueuedAudio() {
    if (audioQueue.length === 0 || !outputAudioContext || isUserSpeaking) {
      isPlayingAudio = false;
      if (!isUserSpeaking) {
        isAISpeaking = false;
      }
      return;
    }
    
    isPlayingAudio = true;
    showStatusMessage('SPEAKING');
    
    const chunk = audioQueue.shift()!;
    const audioBuffer = outputAudioContext.createBuffer(1, chunk.length, 24000);
    const channelData = audioBuffer.getChannelData(0);
    channelData.set(chunk);
    
    const source = outputAudioContext.createBufferSource();
    source.buffer = audioBuffer;
    source.connect(outputAudioContext.destination);
    
    // Store current source for interruption
    currentAudioSource = source;
    
    // Schedule next chunk when this one ends
    source.onended = () => {
      if (currentAudioSource === source) {
        currentAudioSource = null;
      }
      playQueuedAudio();
    };
    
    // Start playback at the right time to avoid gaps
    if (audioStartTime === 0) {
      audioStartTime = outputAudioContext.currentTime;
    }
    
    source.start(audioStartTime);
    audioStartTime += audioBuffer.duration;
  }
  
  function endCall() {
    isCallActive = false;
    isConnecting = false;
    isUserSpeaking = false;
    isAISpeaking = false;
    currentState = 'idle';
    
    // Stop any current audio playback
    if (currentAudioSource) {
      currentAudioSource.stop();
      currentAudioSource = null;
    }
    
    // Clear audio queue and reset timing
    audioQueue = [];
    isPlayingAudio = false;
    audioStartTime = 0;
    
    // Close WebSocket
    if (ws) {
      ws.close();
      ws = null;
    }
    
    // Stop microphone stream
    if (audioStream) {
      audioStream.getTracks().forEach(track => track.stop());
      audioStream = null;
    }
    
    // Cleanup audio contexts
    if (inputAudioContext) {
      inputAudioContext.close();
      inputAudioContext = null;
    }
    
    if (outputAudioContext) {
      outputAudioContext.close();
      outputAudioContext = null;
    }
    
    voiceProcessor = null;
    
    updateButtonState('idle');
    showStatusMessage('NORMAL');
  }
  
  async function handleVoiceClick() {
    if (isCallActive || isConnecting) {
      endCall();
    } else {
      await startRealtimeCall();
    }
  }
  
  // Utility functions for audio data conversion
  function arrayBufferToBase64(buffer: ArrayBuffer): string {
    const bytes = new Uint8Array(buffer);
    let binary = '';
    for (let i = 0; i < bytes.byteLength; i++) {
      binary += String.fromCharCode(bytes[i]);
    }
    return btoa(binary);
  }
  
  function base64ToArrayBuffer(base64: string): ArrayBuffer {
    const binaryString = atob(base64);
    const bytes = new Uint8Array(binaryString.length);
    for (let i = 0; i < binaryString.length; i++) {
      bytes[i] = binaryString.charCodeAt(i);
    }
    return bytes.buffer;
  }
  
  // Handle user speech start (interruption detection)
  function handleUserSpeechStart() {
    if (!isUserSpeaking && isCallActive) {
      isUserSpeaking = true;
      currentState = 'interrupting';
      
      // Cancel current AI response if speaking
      if (isAISpeaking && ws && ws.readyState === WebSocket.OPEN) {
        ws.send(JSON.stringify({ type: 'response.cancel' }));
      }
      
      // Stop current audio playback immediately
      if (currentAudioSource) {
        currentAudioSource.stop();
        currentAudioSource = null;
      }
      
      // Clear audio queue
      audioQueue = [];
      isPlayingAudio = false;
      isAISpeaking = false;
      audioStartTime = 0;
      
      showStatusMessage('INTERRUPTING');
    }
  }
  
  // Handle user speech end
  function handleUserSpeechEnd() {
    if (isUserSpeaking && isCallActive) {
      isUserSpeaking = false;
      currentState = 'listening';
      
      // Commit the user's audio buffer
      if (ws && ws.readyState === WebSocket.OPEN) {
        ws.send(JSON.stringify({ type: 'input_audio_buffer.commit' }));
      }
      
      showStatusMessage('PROCESSING');
    }
  }
  
  voiceButton.addEventListener('click', handleVoiceClick);
  
  voiceButton.addEventListener('keydown', (e) => {
    const keyEvent = e as KeyboardEvent;
    if (keyEvent.key === 'Enter' || keyEvent.key === ' ') {
      keyEvent.preventDefault();
      handleVoiceClick();
    }
  });
  
  // Cleanup on page unload
  window.addEventListener('beforeunload', () => {
    if (isCallActive) {
      endCall();
    }
  });
}

// Initialize when DOM is ready
if (document.readyState === 'loading') {
  document.addEventListener('DOMContentLoaded', () => {
    initStatusBarNavigation();
    initViewSwitcher();
    initScrollToTop();
    initVoiceCall();
  });
} else {
  initStatusBarNavigation();
  initViewSwitcher();
  initScrollToTop();
  initVoiceCall();
}
</script>